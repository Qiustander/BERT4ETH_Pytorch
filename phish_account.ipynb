{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Demo",
   "id": "7896201a91cee4e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare For the Data",
   "id": "9fbe1edb5aef5a36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T02:42:22.013176Z",
     "start_time": "2024-08-04T02:42:19.836661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import args\n",
    "from pretrain.dataloader import BERT4ETHDataloader\n",
    "from models.model import BERT4ETH\n",
    "from pretrain.trainer import BERT4ETHTrainer\n",
    "import pickle as pkl\n",
    "from pretrain.vocab import FreqVocab\n",
    "\n",
    "args.bizdate= '2024'\n"
   ],
   "id": "5548e8d9a0e1a213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Hyper-parameters============\n",
      "Epoch #: 200\n",
      "Vocab #: 3000000\n",
      "Hidden #: 64\n",
      "Max Length: 100\n",
      "ckpt_dir: outputs/cpkt_local\n",
      "learning_rate: 0.0001\n",
      "Max predictions per seq: 80\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T02:42:36.625010Z",
     "start_time": "2024-08-04T02:42:22.015076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare dataset\n",
    "vocab = FreqVocab()\n",
    "print(\"===========Load Sequence===========\")\n",
    "with open(args.data_dir + \"eoa2seq_\" + args.bizdate + \".pkl\", \"rb\") as f:\n",
    "    eoa2seq = pkl.load(f)\n",
    "\n",
    "print(\"number of target user account:\", len(eoa2seq))\n",
    "vocab.update(eoa2seq)\n",
    "# generate mapping\n",
    "vocab.generate_vocab()\n",
    "\n",
    "# save vocab\n",
    "print(\"token_size:{}\".format(len(vocab.vocab_words)))\n",
    "vocab_file_name = args.data_dir + args.vocab_filename + \".\" + args.bizdate\n",
    "print('vocab pickle file: ' + vocab_file_name)\n",
    "with open(vocab_file_name, 'wb') as output_file:\n",
    "    pkl.dump(vocab, output_file, protocol=2)"
   ],
   "id": "356cb03b11a02641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Load Sequence===========\n",
      "number of target user account: 592414\n",
      "token_size:2381818\n",
      "vocab pickle file: inter_data/vocab.2024\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finetune",
   "id": "a80b5893c740757e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T02:42:52.473652Z",
     "start_time": "2024-08-04T02:42:52.466707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pretrain.dataloader import FineTuneLoader\n",
    "from models.model import FineTuneModel\n",
    "from pretrain.trainer import PhishAccountTrainer\n",
    "import pickle as pkl"
   ],
   "id": "e96da3b38b7b125c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T03:03:23.584596Z",
     "start_time": "2024-08-04T02:48:07.641653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "args.bizdate = '2024'\n",
    "args.num_epochs = 5\n",
    "args.lr = 3e-4\n",
    "\n",
    "vocab_file_name = args.data_dir + args.vocab_filename + \".\" + args.bizdate\n",
    "\n",
    "with open(vocab_file_name, \"rb\") as vocab_file:\n",
    "    vocab = pkl.load(vocab_file)\n",
    "with open(args.data_dir + \"eoa2seq_\" + args.bizdate + \".pkl\", \"rb\") as f:\n",
    "    eoa2seq = pkl.load(f)\n",
    "# dataloader\n",
    "dataloader = FineTuneLoader(args, vocab, eoa2seq)\n",
    "train_loader = dataloader.get_train_loader()\n",
    "\n",
    "# model\n",
    "model = FineTuneModel(args)\n",
    "\n",
    "# tranier\n",
    "trainer = PhishAccountTrainer(args, vocab, model, train_loader)\n",
    "trainer.train()"
   ],
   "id": "211e308fda6fde1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pretrain_model.embedding.token_embed.weight torch.Size([3000000, 64]) torch.float32\n",
      "model.pretrain_model.embedding.value_embed.weight torch.Size([15, 64]) torch.float32\n",
      "model.pretrain_model.embedding.count_embed.weight torch.Size([15, 64]) torch.float32\n",
      "model.pretrain_model.embedding.position_embed.weight torch.Size([100, 64]) torch.float32\n",
      "model.pretrain_model.embedding.io_embed.weight torch.Size([3, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.downstream_net.fc1.weight torch.Size([256, 64]) torch.float32\n",
      "model.downstream_net.fc1.bias torch.Size([256]) torch.float32\n",
      "model.downstream_net.fc2.weight torch.Size([256, 256]) torch.float32\n",
      "model.downstream_net.fc2.bias torch.Size([256]) torch.float32\n",
      "model.downstream_net.out_layer.weight torch.Size([1, 256]) torch.float32\n",
      "model.downstream_net.out_layer.bias torch.Size([1]) torch.float32\n",
      "dense.weight torch.Size([64, 64]) torch.float32\n",
      "dense.bias torch.Size([64]) torch.float32\n",
      "LayerNorm.weight torch.Size([64]) torch.float32\n",
      "LayerNorm.bias torch.Size([64]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 853, loss 0.003531 : 100%|██████████| 853/853 [02:57<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: outputs/cpkt_local_phish/epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 1706, loss 0.004285 : 100%|██████████| 853/853 [02:44<00:00,  5.19it/s]\n",
      "Epoch 3, Step 2559, loss 0.000323 : 100%|██████████| 853/853 [02:52<00:00,  4.95it/s]\n",
      "Epoch 4, Step 3412, loss 0.001419 : 100%|██████████| 853/853 [03:17<00:00,  4.33it/s]\n",
      "Epoch 5, Step 4265, loss 0.000079 : 100%|██████████| 853/853 [02:59<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: outputs/cpkt_local_phish/epoch_5.pth\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Phish Account Testing",
   "id": "4c8b518bbec92067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T03:18:45.876104Z",
     "start_time": "2024-08-04T03:18:45.870138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import  os\n",
    "import torch"
   ],
   "id": "9b36fa26d3fd5a9c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T03:19:55.086455Z",
     "start_time": "2024-08-04T03:18:47.239836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "vocab_file_name = args.data_dir + args.vocab_filename + \".\" + args.bizdate\n",
    "\n",
    "with open(vocab_file_name, \"rb\") as vocab_file:\n",
    "    vocab = pkl.load(vocab_file)\n",
    "with open(args.data_dir + \"eoa2seq_\" + args.bizdate + \".pkl\", \"rb\") as f:\n",
    "    eoa2seq = pkl.load(f)\n",
    "# dataloader\n",
    "dataloader = FineTuneLoader(args, vocab, eoa2seq)\n",
    "\n",
    "# model\n",
    "model = FineTuneModel(args)\n",
    "\n",
    "# tranier\n",
    "test_loader = dataloader.get_eval_loader()\n",
    "trainer = PhishAccountTrainer(args, vocab, model, test_loader)\n",
    "\n",
    "# load ckpt\n",
    "ckpt_dir = args.ckpt_dir + \"_phish\"\n",
    "content = os.listdir(ckpt_dir)\n",
    "full_path = [os.path.join(ckpt_dir, x)  for x in content]\n",
    "dir_content = sorted(full_path, key=lambda t: os.stat(t).st_mtime)\n",
    "if not len(dir_content):\n",
    "    raise FileNotFoundError(\"CKPT file for testing needed\")\n",
    "\n",
    "ckpt_dir = dir_content[-1]\n",
    "print(f\"load ckpt at: {ckpt_dir}\")\n",
    "\n",
    "trainer.model.load_state_dict(torch.load(ckpt_dir))\n",
    "\n",
    "final_output, original_data = trainer.predict_proba(test_loader)\n",
    "y_test_proba = np.concatenate(final_output)\n",
    "y_test = np.concatenate(original_data)\n",
    "\n",
    "for threshold in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]:\n",
    "    print(\"threshold =\", threshold)\n",
    "    y_pred = np.zeros_like(y_test_proba)\n",
    "    y_pred[np.where(np.array(y_test_proba) >= threshold)[0]] = 1\n",
    "    print(np.sum(y_pred))\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ],
   "id": "4631400bf1b6dcf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pretrain_model.embedding.token_embed.weight torch.Size([3000000, 64]) torch.float32\n",
      "model.pretrain_model.embedding.value_embed.weight torch.Size([15, 64]) torch.float32\n",
      "model.pretrain_model.embedding.count_embed.weight torch.Size([15, 64]) torch.float32\n",
      "model.pretrain_model.embedding.position_embed.weight torch.Size([100, 64]) torch.float32\n",
      "model.pretrain_model.embedding.io_embed.weight torch.Size([3, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.0.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.1.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.2.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.3.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.4.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.5.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.6.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.0.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.0.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.1.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.1.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.2.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.linear_layers.2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.output_linear.weight torch.Size([64, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.attention.output_linear.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_1.weight torch.Size([256, 64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_1.bias torch.Size([256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_2.weight torch.Size([64, 256]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.feed_forward.w_2.bias torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.input_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.input_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.output_sublayer.norm.a_2 torch.Size([64]) torch.float32\n",
      "model.pretrain_model.transformer_blocks.7.output_sublayer.norm.b_2 torch.Size([64]) torch.float32\n",
      "model.downstream_net.fc1.weight torch.Size([256, 64]) torch.float32\n",
      "model.downstream_net.fc1.bias torch.Size([256]) torch.float32\n",
      "model.downstream_net.fc2.weight torch.Size([256, 256]) torch.float32\n",
      "model.downstream_net.fc2.bias torch.Size([256]) torch.float32\n",
      "model.downstream_net.out_layer.weight torch.Size([1, 256]) torch.float32\n",
      "model.downstream_net.out_layer.bias torch.Size([1]) torch.float32\n",
      "dense.weight torch.Size([64, 64]) torch.float32\n",
      "dense.bias torch.Size([64]) torch.float32\n",
      "LayerNorm.weight torch.Size([64]) torch.float32\n",
      "LayerNorm.bias torch.Size([64]) torch.float32\n",
      "load ckpt at: outputs/cpkt_local_phish/epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:31<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.1\n",
      "1515.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9994    0.9988    0.9991    185637\n",
      "           1     0.8475    0.9257    0.8849      1387\n",
      "\n",
      "    accuracy                         0.9982    187024\n",
      "   macro avg     0.9235    0.9622    0.9420    187024\n",
      "weighted avg     0.9983    0.9982    0.9983    187024\n",
      "\n",
      "threshold = 0.15\n",
      "1473.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9994    0.9990    0.9992    185637\n",
      "           1     0.8690    0.9229    0.8951      1387\n",
      "\n",
      "    accuracy                         0.9984    187024\n",
      "   macro avg     0.9342    0.9609    0.9471    187024\n",
      "weighted avg     0.9985    0.9984    0.9984    187024\n",
      "\n",
      "threshold = 0.2\n",
      "1444.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9994    0.9991    0.9992    185637\n",
      "           1     0.8809    0.9171    0.8986      1387\n",
      "\n",
      "    accuracy                         0.9985    187024\n",
      "   macro avg     0.9401    0.9581    0.9489    187024\n",
      "weighted avg     0.9985    0.9985    0.9985    187024\n",
      "\n",
      "threshold = 0.25\n",
      "1426.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9994    0.9991    0.9992    185637\n",
      "           1     0.8885    0.9135    0.9008      1387\n",
      "\n",
      "    accuracy                         0.9985    187024\n",
      "   macro avg     0.9439    0.9563    0.9500    187024\n",
      "weighted avg     0.9985    0.9985    0.9985    187024\n",
      "\n",
      "threshold = 0.3\n",
      "1415.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9992    0.9993    185637\n",
      "           1     0.8940    0.9120    0.9029      1387\n",
      "\n",
      "    accuracy                         0.9985    187024\n",
      "   macro avg     0.9467    0.9556    0.9511    187024\n",
      "weighted avg     0.9986    0.9985    0.9986    187024\n",
      "\n",
      "threshold = 0.35\n",
      "1402.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9992    0.9993    185637\n",
      "           1     0.9001    0.9099    0.9050      1387\n",
      "\n",
      "    accuracy                         0.9986    187024\n",
      "   macro avg     0.9497    0.9546    0.9521    187024\n",
      "weighted avg     0.9986    0.9986    0.9986    187024\n",
      "\n",
      "threshold = 0.4\n",
      "1399.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9993    0.9993    185637\n",
      "           1     0.9021    0.9099    0.9060      1387\n",
      "\n",
      "    accuracy                         0.9986    187024\n",
      "   macro avg     0.9507    0.9546    0.9526    187024\n",
      "weighted avg     0.9986    0.9986    0.9986    187024\n",
      "\n",
      "threshold = 0.45\n",
      "1390.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9993    0.9993    185637\n",
      "           1     0.9079    0.9099    0.9089      1387\n",
      "\n",
      "    accuracy                         0.9986    187024\n",
      "   macro avg     0.9536    0.9546    0.9541    187024\n",
      "weighted avg     0.9986    0.9986    0.9986    187024\n",
      "\n",
      "threshold = 0.5\n",
      "1388.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9993    0.9993    185637\n",
      "           1     0.9092    0.9099    0.9095      1387\n",
      "\n",
      "    accuracy                         0.9987    187024\n",
      "   macro avg     0.9543    0.9546    0.9544    187024\n",
      "weighted avg     0.9987    0.9987    0.9987    187024\n",
      "\n",
      "threshold = 0.55\n",
      "1380.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9993    0.9993    185637\n",
      "           1     0.9123    0.9077    0.9100      1387\n",
      "\n",
      "    accuracy                         0.9987    187024\n",
      "   macro avg     0.9558    0.9535    0.9547    187024\n",
      "weighted avg     0.9987    0.9987    0.9987    187024\n",
      "\n",
      "threshold = 0.6\n",
      "1374.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9994    0.9993    185637\n",
      "           1     0.9156    0.9070    0.9113      1387\n",
      "\n",
      "    accuracy                         0.9987    187024\n",
      "   macro avg     0.9574    0.9532    0.9553    187024\n",
      "weighted avg     0.9987    0.9987    0.9987    187024\n",
      "\n",
      "threshold = 0.65\n",
      "1367.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9994    0.9994    185637\n",
      "           1     0.9203    0.9070    0.9136      1387\n",
      "\n",
      "    accuracy                         0.9987    187024\n",
      "   macro avg     0.9598    0.9532    0.9565    187024\n",
      "weighted avg     0.9987    0.9987    0.9987    187024\n",
      "\n",
      "threshold = 0.7\n",
      "1354.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9993    0.9995    0.9994    185637\n",
      "           1     0.9254    0.9034    0.9143      1387\n",
      "\n",
      "    accuracy                         0.9987    187024\n",
      "   macro avg     0.9623    0.9514    0.9568    187024\n",
      "weighted avg     0.9987    0.9987    0.9987    187024\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let us try one account",
   "id": "40dac8f7d715096e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T02:26:01.990705Z",
     "start_time": "2024-08-05T02:26:01.686163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = next(iter(test_loader))\n",
    "print(f\"Length of batch {len(batch[0])}\")"
   ],
   "id": "66824c5756e4a19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of batch 1024\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T04:33:45.071070Z",
     "start_time": "2024-08-05T04:33:45.051445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get one random account\n",
    "# indx = np.random.randint(len(batch[0]))\n",
    "indx = 1021\n",
    "data = [x[indx].to('cuda') for x in batch]\n",
    "print(data)"
   ],
   "id": "889ca7272a32647c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([358076], device='cuda:0'), tensor([ 358076, 2326570, 2326571,      33, 2326572,      10,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0], device='cuda:0'), tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor([ 0, 11, 10, 10,  9,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0'), tensor([0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor([0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor([1], device='cuda:0')]\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T04:33:46.071163Z",
     "start_time": "2024-08-05T04:33:46.046320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "trainer.model.eval()\n",
    "input_ids = data[1][None]\n",
    "counts = data[2][None]\n",
    "values = data[3][None]\n",
    "io_flags = data[4][None]\n",
    "positions = data[5][None]\n",
    "logits = trainer.model(input_ids, counts, values, io_flags, positions)\n",
    "y_prob = F.sigmoid(logits).detach().cpu().numpy()"
   ],
   "id": "ed8522aa7d335172",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T04:33:46.775820Z",
     "start_time": "2024-08-05T04:33:46.770072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"probability of the prediction {y_prob}\")\n",
    "print(f\"true label{data[-1].cpu().numpy()}\")\n"
   ],
   "id": "cd3e7a338ffe021d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the prediction [[0.9999988]]\n",
      "true label[1]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T04:33:48.056924Z",
     "start_time": "2024-08-05T04:33:48.051891Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "daf89cfb8347dfda",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3fc082fe65d64ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
